\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb}

\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}

\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\begin{document}

\title{Homework 1}
\author{Aarsh Patel\\
CS687: Reinforcement Learning}

\maketitle

\section*{Written}

\begin{problem}{1}
\text{Aarsh Patel}\\
\end{problem}

\begin{problem}{2}
\text{ }\\
\end{problem}

\begin{problem}{3}
\text{ }\\
\end{problem}

\begin{problem}{4}
\text{ }\\
The number of stochastic policies when $|S| < \infty$ and $|A| < \infty$ is $\infty$. There are essentially
inifinite number of ways to fill the $S$ x $A$ tabular policy matrix. The only contraint is that the rows
need to add up to 1
\end{problem}

\begin{problem}{5}
\text{ }\\
\end{problem}

\begin{problem}{6}
\text{ }\\
\end{problem}

\begin{problem}{7}
\text{ }\\
\end{problem}

\begin{problem}{8}
\text{ }\\
\end{problem}

\begin{problem}{9}
\text{ }\\
\end{problem}

\begin{problem}{10}
\text{ }\\
Since $S_{\infty}$ $\in$ $S$, we just create a markov chain with some states and have a state that transitions to $S_{\infty}$ with some probability,
but make sure that probability is not 1. Thus this state couldn't be considered a terminal state since it doesn't transition to $S_{\infty}$ with probability
1. However this MDP will always terminate at some point since there is exists a state that transitions to $S_{\infty}$ with some probability.
\end{problem}

\begin{problem}{11}
\text{ }\\
Create an Markov Chain with three states that transition in a cycle with probability 1. Thus all states
in that markov chain have a period of 3. Just need to convert the markov chain into MDP with rewards, actions and intial state.
\end{problem}

\begin{problem}{12}
\text{ }\\
\end{problem}

\begin{problem}{13}
\text{ }\\

For a MDP to not be ergodic we need to find a markov chain that associated with a determinstic policy that is not aperiodic or positive
recurrent. Thus what we can do is to have a determinstic policy's markov chain be peroidic in the sense that one state is periodic.

Have 3 states, states 1 and 2 sort of loop back to each other and state 2 transitions to Terminal state with probability .1. There determinstic policy
within this is 1->2->1->2 continous loop, that the markov chain associated with this have states 1 and 2 have periods of 2 and thus the markov chain is
not aperodic. Thus the MDP is not ergodic.

You can have two actions L, R and have make a 2 x 2 table with determinstic actions. Then create the markov chain from this determinstic policy
and show that the markov chain has states with peroid of 2, thus this chain is not aperiodic and thus the MDP is ergodic.

\end{problem}

\begin{problem}{14}
\text{ }\\
\end{problem}

\begin{problem}{15}
\text{ }\\
We can think of Golf as a real world problem where $R_t$ is determinstic of $S_t$. It doesn't matter which action or new state you land up on,
you are always going to get a -1 reward when you take some action in a state. Thus the reward is determinstic of the state. The goal of golf
is to play as few strokes (actions) as possible.

Model Golf MDP \dots
\end{problem}

\begin{problem}{16}
\text{ }\\
\end{problem}

\begin{problem}{17}
\text{ }\\
Atari Game, Brick Breaker. The agent knows the reward beforehand (rubik cube, you know the rewards beforehand)
\end{problem}

\begin{problem}{18}
\text{ }\\
Pong, you don't know the reward function, because you don't know if the action you take in a certain state will you get you a reward of +1 if you get
it past the opponent.
\end{problem}

\begin{problem}{19}
\text{ }\\
Trying to solve the rubrik cube. When you take a action in a state you know the resulting state.
\end{problem}

\begin{problem}{20}
\text{ }\\

We can model the sport \textbf{Golf} as an MDP where the transition function would not be known. When you are in state, for example,
in the Sand, and you hit the golf ball with a power shot (action) you don't know what state your golf ball will land in. There may be other factors affecting
the direction/velocity of the golf ball. Thus you don't know the full transition function in golf since you don't really know beforehand
where your golf ball will land when you hit it.

States = {Tee, Fairway, Greens, Hazards, Rough, Cup}
Actions = {Drive, Putt}
Initial State = Tee
Transition Model = Don't know the transition model beforehand
Reward Function = -1 for all states. You want to reach the cup as quickly as possible.

\end{problem}

\section*{Programming}

\begin{problem}{1}
\text{ }\\
\end{problem}

\end{document}
